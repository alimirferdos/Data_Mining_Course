{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"DataMiningHW2_Q4.ipynb","provenance":[],"collapsed_sections":["dYktuWzl5-79","rJ1Qy-JpNyeB","g_o1m1VeheO0"],"authorship_tag":"ABX9TyM6ZmzYGn9ZFqUBeNRSeulG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sentence Emotion Prediction Using Frequent Itemset Mining"],"metadata":{"id":"P0rLvLd4yima"}},{"cell_type":"markdown","source":["Name: Seyed Ali Mirferdos\n","\n","Student Number: 99201465"],"metadata":{"id":"6cdT0BcDyztd"}},{"cell_type":"markdown","source":["# Importing required modules"],"metadata":{"id":"h0zJR0mO7fnK"}},{"cell_type":"code","execution_count":null,"source":["!pip install pyspark"],"outputs":[],"metadata":{"id":"xghaUThUMiUa"}},{"cell_type":"code","execution_count":145,"source":["import pandas as pd\n","import numpy as np\n","import string\n","import nltk\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from textblob import TextBlob\n","import re\n","from collections import defaultdict\n","import itertools\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession"],"outputs":[],"metadata":{"id":"QtslvScW7qRB","executionInfo":{"status":"ok","timestamp":1617391961152,"user_tz":-270,"elapsed":673,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":121,"source":["nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":121}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOZL2REzCBOO","executionInfo":{"status":"ok","timestamp":1617387883477,"user_tz":-270,"elapsed":1187,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"8a811c55-5f8f-4350-9518-8aceef48522d"}},{"cell_type":"markdown","source":["# Reading the data"],"metadata":{"id":"9_GW9cPF6y3K"}},{"cell_type":"code","execution_count":null,"source":["!unzip q4.csv.zip"],"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  q4.csv.zip\n","  inflating: train.csv               \n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xlg20xcO61Id","executionInfo":{"status":"ok","timestamp":1617361092422,"user_tz":-270,"elapsed":1143,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"66b9ba9b-b482-4270-c83e-4d6c6f5004fa"}},{"cell_type":"code","execution_count":109,"source":["df = pd.read_csv('train.csv', index_col=0)"],"outputs":[],"metadata":{"id":"hOX9JYFx88SC","executionInfo":{"status":"ok","timestamp":1617387784333,"user_tz":-270,"elapsed":1548,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":110,"source":["df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["       id                                               text emotions\n","0   27383  i feel awful about it too because it s my job ...  sadness\n","1  110083                              im alone i feel awful  sadness\n","2  140764  ive probably mentioned this before but i reall...      joy\n","3  100071           i was feeling a little low few days back  sadness\n","4    2837  i beleive that i am much more sensitive to oth...     love"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>27383</td>\n","      <td>i feel awful about it too because it s my job ...</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>110083</td>\n","      <td>im alone i feel awful</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>140764</td>\n","      <td>ive probably mentioned this before but i reall...</td>\n","      <td>joy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100071</td>\n","      <td>i was feeling a little low few days back</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2837</td>\n","      <td>i beleive that i am much more sensitive to oth...</td>\n","      <td>love</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{"tags":[]},"execution_count":110}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"QdeZW4GH9DBp","executionInfo":{"status":"ok","timestamp":1617387784335,"user_tz":-270,"elapsed":1265,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"414fc4b9-4810-43a6-c6e5-504a9a86039d"}},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"hNJUtVIG6k-Y"}},{"cell_type":"markdown","source":["This part is completely taken from the [link](https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908) provided in the question:"],"metadata":{"id":"oisPdnxHzC_P"}},{"cell_type":"markdown","source":["## 1. Lowercase"],"metadata":{"id":"LaDLbcg19noS"}},{"cell_type":"code","execution_count":111,"source":["df['text'] = df['text'].map(str.lower)"],"outputs":[],"metadata":{"id":"ga0PmPu_5UtX","executionInfo":{"status":"ok","timestamp":1617387790499,"user_tz":-270,"elapsed":1022,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 2. Removing numbers"],"metadata":{"id":"lvNu4-l4-nxb"}},{"cell_type":"code","execution_count":112,"source":["df['text'] = df['text'].map(lambda x: re.sub(r'\\d+', '', x))"],"outputs":[],"metadata":{"id":"TKJwGd3e-35m","executionInfo":{"status":"ok","timestamp":1617387792243,"user_tz":-270,"elapsed":1985,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 3. Removing all non-ascii characters"],"metadata":{"id":"HZg6IUTs_HR0"}},{"cell_type":"code","execution_count":113,"source":["df['text'] = df['text'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))"],"outputs":[],"metadata":{"id":"xnmy9H-h_FLS","executionInfo":{"status":"ok","timestamp":1617387793115,"user_tz":-270,"elapsed":2119,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 4. Removing punctuation"],"metadata":{"id":"_wd1i4jM_4Vq"}},{"cell_type":"code","execution_count":114,"source":["df['text'] = df['text'].map(lambda x: x.translate(str.maketrans('','', string.punctuation)))"],"outputs":[],"metadata":{"id":"2MbiY8tqA4BA","executionInfo":{"status":"ok","timestamp":1617387794788,"user_tz":-270,"elapsed":2386,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 5. Removing stop words"],"metadata":{"id":"icoEXKmFBOh3"}},{"cell_type":"code","execution_count":115,"source":["stop_words = set(stopwords.words('english'))"],"outputs":[],"metadata":{"id":"2qpJn0ayB7pR","executionInfo":{"status":"ok","timestamp":1617387794790,"user_tz":-270,"elapsed":1162,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":116,"source":["df['text'] = df['text'].map(lambda x: \n","                            [i for i in word_tokenize(x) if not i in stop_words])"],"outputs":[],"metadata":{"id":"o4StvCTHBcfN","executionInfo":{"status":"ok","timestamp":1617387841705,"user_tz":-270,"elapsed":45278,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 6. Removing single letter words"],"metadata":{"id":"6cWFkYX9DhvL"}},{"cell_type":"code","execution_count":117,"source":["df['text'] = df['text'].map(lambda x: \n","                            [w for w in x if len(w) > 1])"],"outputs":[],"metadata":{"id":"SUnlCWzXDmAg","executionInfo":{"status":"ok","timestamp":1617387842839,"user_tz":-270,"elapsed":44892,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 7. Lemmatizing Words"],"metadata":{"id":"RrBellgPEp_P"}},{"cell_type":"markdown","source":["This is part is taken from the third method discussed [here](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#wordnetlemmatizerwithappropriatepostag):"],"metadata":{"id":"RqcNp4fT1S6E"}},{"cell_type":"code","execution_count":118,"source":["def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    return tag_dict.get(tag, wordnet.NOUN)"],"outputs":[],"metadata":{"id":"E8elFeiL01C9","executionInfo":{"status":"ok","timestamp":1617387844940,"user_tz":-270,"elapsed":820,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":119,"source":["lemmatizer = WordNetLemmatizer()"],"outputs":[],"metadata":{"id":"mdh_G4EaE_aq","executionInfo":{"status":"ok","timestamp":1617387866175,"user_tz":-270,"elapsed":906,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":122,"source":["df['text'] = df['text'].map(lambda x: \n","                            [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in x])"],"outputs":[],"metadata":{"id":"LtSu0_W0FBfJ","executionInfo":{"status":"ok","timestamp":1617388339654,"user_tz":-270,"elapsed":449321,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 8. Word Counts"],"metadata":{"id":"mXRZup-VFsra"}},{"cell_type":"code","execution_count":123,"source":["df.head()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["       id                                               text emotions\n","0   27383  [feel, awful, job, get, position, succeed, hap...  sadness\n","1  110083                           [im, alone, feel, awful]  sadness\n","2  140764  [ive, probably, mention, really, feel, proud, ...      joy\n","3  100071                     [feel, little, low, day, back]  sadness\n","4    2837  [beleive, much, sensitive, people, feeling, te...     love"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>27383</td>\n","      <td>[feel, awful, job, get, position, succeed, hap...</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>110083</td>\n","      <td>[im, alone, feel, awful]</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>140764</td>\n","      <td>[ive, probably, mention, really, feel, proud, ...</td>\n","      <td>joy</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100071</td>\n","      <td>[feel, little, low, day, back]</td>\n","      <td>sadness</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2837</td>\n","      <td>[beleive, much, sensitive, people, feeling, te...</td>\n","      <td>love</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{"tags":[]},"execution_count":123}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"lGcHReBcJexr","executionInfo":{"status":"ok","timestamp":1617388348093,"user_tz":-270,"elapsed":830,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"02d02099-48c9-4ff5-9da6-a803c2c19bf6"}},{"cell_type":"code","execution_count":135,"source":["word_total_count = df.explode('text').dropna().value_counts(subset=['text'])"],"outputs":[],"metadata":{"id":"xWFBdhw2SYvS","executionInfo":{"status":"ok","timestamp":1617388917632,"user_tz":-270,"elapsed":3328,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["For getting the emotion count I thought about different methods and asked for advice from some friends and I finally came up with two methods which I've commented one:"],"metadata":{"id":"PgXALIWT3xnD"}},{"cell_type":"code","execution_count":128,"source":["word_emotion_count = df.explode('text').dropna().pivot_table(index=\"text\", \n","                                                             columns='emotions',\n","                                                             aggfunc=len,\n","                                                             fill_value=0)"],"outputs":[],"metadata":{"id":"NRBLP3We3dIv","executionInfo":{"status":"ok","timestamp":1617388684354,"user_tz":-270,"elapsed":4492,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":129,"source":["word_emotion_count.columns = word_emotion_count.columns.droplevel()"],"outputs":[],"metadata":{"id":"mWwr8q0w2SzQ","executionInfo":{"status":"ok","timestamp":1617388691947,"user_tz":-270,"elapsed":728,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":136,"source":["word_total_count = word_total_count.to_frame(name='total_count')"],"outputs":[],"metadata":{"id":"TKULFftQ2SzR","executionInfo":{"status":"ok","timestamp":1617388927576,"user_tz":-270,"elapsed":662,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":137,"source":["words = pd.merge(word_total_count, word_emotion_count, on='text')"],"outputs":[],"metadata":{"id":"Rj3cWILu2SzR","executionInfo":{"status":"ok","timestamp":1617388929665,"user_tz":-270,"elapsed":818,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":null,"source":["# s = df.groupby(\"emotions\")[\"text\"]\\\n","#        .apply(lambda x: Counter(t for texts in x for t in texts))\\\n","#        .dropna()\\\n","#        .astype(int)\n","# s.unstack(level=0)"],"outputs":[],"metadata":{"id":"5pmcPMtO2SzQ"}},{"cell_type":"markdown","source":["# Requests"],"metadata":{"id":"tZxCtyjfDcPP"}},{"cell_type":"markdown","source":["## 1. Saving the list of words"],"metadata":{"id":"2y9dRAYw5aId"}},{"cell_type":"code","execution_count":143,"source":["pd.Series(words.index).to_csv('words.txt', header=False, index=False)"],"outputs":[],"metadata":{"id":"s2bkzCNv5xFM","executionInfo":{"status":"ok","timestamp":1617389362969,"user_tz":-270,"elapsed":726,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["## 2. Memory needed for all frequent itemsets"],"metadata":{"id":"QTZZ5Dsx60-Z"}},{"cell_type":"markdown","source":["The total number of words extracted is: 52210\n","\n","Let's assume each word takes 2 bytes of memory.\n","\n","For the single-sets we'll have 52210 * 2 = 104.42 kilobytes.\n","\n","For the pair-sets we'll have 52210 * 52209 = 2.7 gigabytes\n","\n","For the three-sets we'll have 52210 * 52209 * 52208 * 2/6 = 47.4 terabytes\n","\n","In the case we want to have all the itemsets it would be $2^{52211}$ bytes."],"metadata":{"id":"8WYGoNrR7WO8"}},{"cell_type":"markdown","source":["## 3. How much itemsets can be handled in 16GB memory"],"metadata":{"id":"d0q7RFvt7OED"}},{"cell_type":"markdown","source":["As we saw in the previous section we'll have around 2.8 gigabytes for the single and double sets. So in the 16GB memory, 13.2GB will be left.\n","\n","All the three-sets can be handled in a 47.4 terabytes memory so the fraction which can be handled in a 13.2GB memory is 13.2GB*(number of 3 sets)/ (2 * 47.4TB) which would be 66 * $10^8$ three-set itemsets.\n","\n","So we can put all the single-sets, pair-sets and 66 * $10^8$ of three-sets in a 16GB memory."],"metadata":{"id":"dEtX32GB7Vy-"}},{"cell_type":"markdown","source":["## 4. SON algorithm"],"metadata":{"id":"fpkWNYiJ7Aqi"}},{"cell_type":"markdown","source":["The main SON algorithm for MapReduce is taken from [here](https://www.geeksforgeeks.org/the-son-algorithm-and-map-reduce/).\n","\n","Most of the code of MapReduce is taken from [here](https://github.com/nityapydipati/SON-Algorithm-using-Apache-Spark/blob/master/Nitya_Pydipati_son.py)."],"metadata":{"id":"XUdAfrdGEQok"}},{"cell_type":"code","execution_count":null,"source":["sc = SparkContext()\n","spark = SparkSession(sc)"],"outputs":[],"metadata":{"id":"-wstGG0LDjoU"}},{"cell_type":"markdown","source":["rdd1 will be a rdd made from the main DataFrame:"],"metadata":{"id":"FBrw52LmMquq"}},{"cell_type":"code","execution_count":171,"source":["rdd1 = spark.createDataFrame(df).rdd"],"outputs":[],"metadata":{"id":"90VvL9ncD6cP","executionInfo":{"status":"ok","timestamp":1617393696320,"user_tz":-270,"elapsed":12954,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["Calculating the thresholds:"],"metadata":{"id":"Td-FyIVMMyea"}},{"cell_type":"code","execution_count":172,"source":["s_th = 0.05 * len(df)\n","partitions = 500"],"outputs":[],"metadata":{"id":"rxvV2QYZGT0b","executionInfo":{"status":"ok","timestamp":1617393696321,"user_tz":-270,"elapsed":11999,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":173,"source":["partition_s_th = s_th / partitions"],"outputs":[],"metadata":{"id":"O1qz-M1LGayK","executionInfo":{"status":"ok","timestamp":1617393696325,"user_tz":-270,"elapsed":11838,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["Setting the number of partitions:"],"metadata":{"id":"nypeeG2NM1kl"}},{"cell_type":"code","execution_count":174,"source":["rdd2 = rdd1.coalesce(partitions, True)"],"outputs":[],"metadata":{"id":"3yaB3aM5MW3U","executionInfo":{"status":"ok","timestamp":1617393696327,"user_tz":-270,"elapsed":11331,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":175,"source":["rdd2.getNumPartitions()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["500"]},"metadata":{"tags":[]},"execution_count":175}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx1cmRCCTea1","executionInfo":{"status":"ok","timestamp":1617393696329,"user_tz":-270,"elapsed":11212,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"79aafadf-cf81-4a71-8dc8-fae8cd14ca02"}},{"cell_type":"markdown","source":["rdd2 will only have the text column of each row:"],"metadata":{"id":"9Ae56pKNM7Xw"}},{"cell_type":"code","execution_count":176,"source":["rdd2 = rdd2.map(lambda b: b['text'])"],"outputs":[],"metadata":{"id":"QhiqlIy4EnIz","executionInfo":{"status":"ok","timestamp":1617393696331,"user_tz":-270,"elapsed":11095,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["Finding the frequent sets in a list of baskets:"],"metadata":{"id":"9Ckp-ly1M_mR"}},{"cell_type":"code","execution_count":177,"source":["def frequent(frequent_sets,baskets,sup):\n","    frequent_dict = defaultdict(int)\n","    for item in frequent_sets:\n","        for basket in baskets:\n","            if item.issubset(basket):\n","                frequent_dict[frozenset(item)] += 1\n","    items=set()\n","    for item in frequent_dict:\n","        if frequent_dict[item] >= sup:\n","            items.add(item)\n","    return items"],"outputs":[],"metadata":{"id":"OnRRSALCKbXx","executionInfo":{"status":"ok","timestamp":1617393703554,"user_tz":-270,"elapsed":841,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["Main Apriori algorithm for single and pair frequent sets:"],"metadata":{"id":"yxEd1OGhND1j"}},{"cell_type":"code","execution_count":178,"source":["def apriori(items):\n","    freq_one = defaultdict(int)\n","    baskets = []\n","    frequent_sets = set()\n","    results = dict()\n","    for item in items:\n","        baskets.append(item)\n","        for i in item:\n","            freq_one[i]+=1\n","    for freq in freq_one:\n","        if(freq_one[freq]>=partition_s_th):\n","            frequent_sets.add(freq)\n","    \n","    results[1]=[frozenset([item]) for item in frequent_sets]\n","    combine=set()\n","    for sets in frequent_sets:\n","        combine.add(sets)\n","    sets=[set(sorted(item)) for item in \n","          itertools.chain(*[itertools.combinations(combine, 2)])]\n","    frequent_sets=sets\n","    next_freq=frequent(frequent_sets,baskets,partition_s_th)\n","    if (next_freq): \n","      results[2]=next_freq\n","    frequent_sets=next_freq\n","    return results"],"outputs":[],"metadata":{"id":"ISF9XOXl5rjl","executionInfo":{"status":"ok","timestamp":1617393706817,"user_tz":-270,"elapsed":1631,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["MapReduced SON Phase 1:"],"metadata":{"id":"yVWTtuHENJ_W"}},{"cell_type":"code","execution_count":179,"source":["basket=rdd2.mapPartitions(lambda line: [y for y in apriori(line).values()])\n","\n","map_one=basket.flatMap(lambda x: [(y, 1) for y in x])\n","\n","reduce_one=map_one.reduceByKey(lambda x, y: x)\n","\n","item_red=reduce_one.map(lambda x: x[0]).collect()"],"outputs":[],"metadata":{"id":"HYECJl2A5039","executionInfo":{"status":"ok","timestamp":1617393756558,"user_tz":-270,"elapsed":47561,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["MapReduced SON Phase 2:"],"metadata":{"id":"g6hfDbIXNOoa"}},{"cell_type":"code","execution_count":180,"source":["broadcasting_global_count=sc.broadcast(item_red)\n","\n","map_two=rdd2.flatMap(lambda line: [(count,1) for count in \n","                                   broadcasting_global_count.value \n","                                   if set(line).issuperset(set(count))])\n","reduce_two=map_two.reduceByKey(lambda x,y: x+y)\n","\n","global_count=reduce_two.filter(lambda x: x[1]>=s_th)\n","output=global_count.collect()"],"outputs":[],"metadata":{"id":"JxuHROfO6c0K","executionInfo":{"status":"ok","timestamp":1617393795747,"user_tz":-270,"elapsed":39181,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"markdown","source":["As we can see, we have the frequent itemsets and their total count in the dataset:"],"metadata":{"id":"3JhTS-mDNRFn"}},{"cell_type":"code","execution_count":181,"source":["output"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(frozenset({'know'}), 18037),\n"," (frozenset({'im'}), 39047),\n"," (frozenset({'feel', 'time'}), 16749),\n"," (frozenset({'make'}), 20315),\n"," (frozenset({'like'}), 50467),\n"," (frozenset({'feel', 'im'}), 38484),\n"," (frozenset({'feel', 'go'}), 18535),\n"," (frozenset({'feel'}), 291534),\n"," (frozenset({'feel', 'like'}), 49984),\n"," (frozenset({'feel', 'make'}), 19937),\n"," (frozenset({'time'}), 17215),\n"," (frozenset({'feel', 'get'}), 21349),\n"," (frozenset({'go'}), 19162),\n"," (frozenset({'really'}), 17247),\n"," (frozenset({'feel', 'know'}), 17555),\n"," (frozenset({'feel', 'really'}), 16927),\n"," (frozenset({'get'}), 22115)]"]},"metadata":{"tags":[]},"execution_count":181}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWYSsWve7lW3","executionInfo":{"status":"ok","timestamp":1617393795750,"user_tz":-270,"elapsed":39154,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"793ac04a-8dce-4282-df41-7434ca02355c"}},{"cell_type":"markdown","source":["Now we want to calculate the count of frequent itemsets in each emotion:"],"metadata":{"id":"dKoZgM0QNYIi"}},{"cell_type":"code","execution_count":183,"source":["freq_itemsets = [i[0] for i in output]"],"outputs":[],"metadata":{"id":"YhVnOH3vKsYT","executionInfo":{"status":"ok","timestamp":1617393805883,"user_tz":-270,"elapsed":958,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":null,"source":["def get_freq_itemset_emotions(row):\n","  return [(row['emotions'], item_set) for item_set in freq_items \n","            if item_set.issubset(row['text'])]\n","\n","mapped = rdd1.map(lambda row: get_freq_itemset_emotions(row))\\\n","            .flatMap(lambda x: x)\\\n","            .map(lambda x: (x, 1))\\\n","            .reduceByKey(lambda x,y: x+y)\n","mapped.collect()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('joy', frozenset({'feel'})), 71984),\n"," (('joy', frozenset({'like'})), 16811),\n"," (('joy', frozenset({'feel', 'like'})), 14701),\n"," (('joy', frozenset({'feeling'})), 30466),\n"," (('anger', frozenset({'time'})), 2423),\n"," (('joy', frozenset({'know'})), 4966),\n"," (('joy', frozenset({'im'})), 12928),\n"," (('joy', frozenset({'feel', 'im'})), 5350),\n"," (('surprise', frozenset({'feeling'})), 3722),\n"," (('surprise', frozenset({'feel'})), 7248),\n"," (('joy', frozenset({'feeling', 'im'})), 7879),\n"," (('sadness', frozenset({'really'})), 5012),\n"," (('surprise', frozenset({'like'})), 1588),\n"," (('surprise', frozenset({'feel', 'like'})), 1357),\n"," (('fear', frozenset({'time'})), 2014),\n"," (('surprise', frozenset({'know'})), 650),\n"," (('surprise', frozenset({'im'})), 1232),\n"," (('surprise', frozenset({'feel', 'im'})), 563),\n"," (('sadness', frozenset({'time'})), 5112),\n"," (('love', frozenset({'really'})), 1491),\n"," (('love', frozenset({'time'})), 1372),\n"," (('fear', frozenset({'really'})), 1870),\n"," (('anger', frozenset({'really'})), 2475),\n"," (('surprise', frozenset({'feeling', 'im'})), 703),\n"," (('sadness', frozenset({'feel'})), 58505),\n"," (('sadness', frozenset({'im'})), 11228),\n"," (('sadness', frozenset({'feel', 'im'})), 5065),\n"," (('joy', frozenset({'really'})), 5738),\n"," (('sadness', frozenset({'feeling'})), 30205),\n"," (('love', frozenset({'feeling'})), 7965),\n"," (('love', frozenset({'feel'})), 17299),\n"," (('joy', frozenset({'time'})), 5600),\n"," (('anger', frozenset({'feel'})), 26372),\n"," (('anger', frozenset({'feeling'})), 14242),\n"," (('fear', frozenset({'feel'})), 21577),\n"," (('fear', frozenset({'like'})), 4218),\n"," (('fear', frozenset({'feel', 'like'})), 3452),\n"," (('sadness', frozenset({'feeling', 'im'})), 6523),\n"," (('anger', frozenset({'im'})), 5912),\n"," (('anger', frozenset({'like'})), 6945),\n"," (('anger', frozenset({'feel', 'im'})), 2440),\n"," (('anger', frozenset({'feel', 'like'})), 5904),\n"," (('sadness', frozenset({'know'})), 5029),\n"," (('sadness', frozenset({'like'})), 14751),\n"," (('sadness', frozenset({'feel', 'like'})), 12409),\n"," (('surprise', frozenset({'really'})), 661),\n"," (('fear', frozenset({'feeling'})), 12855),\n"," (('fear', frozenset({'im'})), 4784),\n"," (('fear', frozenset({'feeling', 'im'})), 2912),\n"," (('anger', frozenset({'know'})), 2347),\n"," (('love', frozenset({'like'})), 5019),\n"," (('love', frozenset({'feel', 'like'})), 4339),\n"," (('fear', frozenset({'know'})), 2105),\n"," (('anger', frozenset({'feeling', 'im'})), 3618),\n"," (('love', frozenset({'im'})), 2963),\n"," (('love', frozenset({'feeling', 'im'})), 1853),\n"," (('fear', frozenset({'feel', 'im'})), 2021),\n"," (('love', frozenset({'know'})), 1431),\n"," (('love', frozenset({'feel', 'im'})), 1163),\n"," (('surprise', frozenset({'time'})), 682)]"]},"metadata":{"tags":[]},"execution_count":39}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLNY4tZE7xSr","executionInfo":{"status":"ok","timestamp":1617361390074,"user_tz":-270,"elapsed":98118,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"413d4c56-cc30-4648-cba9-4ee8cca53190"}},{"cell_type":"markdown","source":["## 5. Proposing a equation for feeling probability of a sentence"],"metadata":{"id":"t6BvWtwXAF8y"}},{"cell_type":"markdown","source":["We suppose each sentence is preprocessed into a sequence of words: w1, w2, ..., wn\n","\n","As locality is discussed in the field of NLP, the combination of words with its neighbours is important thus we approximate the feeling probability of the sentence using the 2-grams of the given sentence. i.e we consider the following 2-grams: (W1, W2), (W2, W3), ..., (Wn-1, Wn).\n","\n","For each pair of words (a, b) we calculate the number of occurences in the whole dataset in addition to occurencces in each emotion. Then we divide the vector of emotion counts by the total count to get the probability of each emotion for the given pair. This gives us an estimate of how this pair would change the sentence's emotional distribution.\n","\n","Then for each 2-gram of the sentence we get the emotion distribution vector and sum them up. Finally, we normalize the final vector by dividing it by the sum of all the values."],"metadata":{"id":"SBQ8I1txN8Z4"}},{"cell_type":"markdown","source":["## 6. Calculating the probability of each sentence in the dataset"],"metadata":{"id":"Ba1eIv71Nype"}},{"cell_type":"code","execution_count":null,"source":["df2 = df.copy()[['text', 'emotions']]"],"outputs":[],"metadata":{"id":"Dkk-P8uZCLor"}},{"cell_type":"markdown","source":["Generating the 2-grams for each sentence in the dataset:"],"metadata":{"id":"f9QqKJbKQUDt"}},{"cell_type":"code","execution_count":184,"source":["def get_2_grams(word_array):\n","  result = []\n","  for i in range (len(word_array)-1):\n","    result.append(tuple((word_array[i], word_array[i+1])))\n","  return result"],"outputs":[],"metadata":{"id":"aENfmjx_8kcO","executionInfo":{"status":"ok","timestamp":1617394940232,"user_tz":-270,"elapsed":746,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}}}},{"cell_type":"code","execution_count":null,"source":["df2['2-grams'] = df['text'].apply(get_2_grams)"],"outputs":[],"metadata":{"id":"nbAOFUUgQDv4"}},{"cell_type":"markdown","source":["Counting the number of each emotion for each 2-gram:"],"metadata":{"id":"84tuCDouQoA8"}},{"cell_type":"code","execution_count":null,"source":["df3 = df2.explode('2-grams')\\\n","         .dropna()\\\n","         .pivot_table(index=\"2-grams\", columns='emotions', \n","                      aggfunc=len, fill_value=0)"],"outputs":[],"metadata":{"id":"by0ke779BoMz"}},{"cell_type":"code","execution_count":null,"source":["df3.columns = df3.columns.droplevel()"],"outputs":[],"metadata":{"id":"y3ADvTL_FPUL"}},{"cell_type":"markdown","source":["Counting the total number of each 2-gram:"],"metadata":{"id":"ilj7RINARAAJ"}},{"cell_type":"code","execution_count":null,"source":["pair_totals = df2.explode('2-grams')\\\n","                 .dropna().value_counts(subset=['2-grams'])"],"outputs":[],"metadata":{"id":"LfuRNxkDK6lk"}},{"cell_type":"code","execution_count":null,"source":["pair_totals = pair_totals.to_frame(name='count')"],"outputs":[],"metadata":{"id":"E5YpzL2iK9SE"}},{"cell_type":"code","execution_count":null,"source":["pair_counts = pd.merge(df3, pair_totals, on='2-grams')"],"outputs":[],"metadata":{"id":"lRGnRFB9LyKp"}},{"cell_type":"markdown","source":["Getting the feeling probability distribution of each 2-gram:"],"metadata":{"id":"2SmjsnkxRPSL"}},{"cell_type":"code","execution_count":null,"source":["pair_counts['anger'] /= pair_counts['count']\n","pair_counts['fear'] /= pair_counts['count']\n","pair_counts['joy'] /= pair_counts['count']\n","pair_counts['love'] /= pair_counts['count']\n","pair_counts['sadness'] /= pair_counts['count']\n","pair_counts['surprise'] /= pair_counts['count']"],"outputs":[],"metadata":{"id":"aVYg68fWVoGw"}},{"cell_type":"markdown","source":["Finally, let's create the final DataFrame:"],"metadata":{"id":"HJNsVXWnRfcj"}},{"cell_type":"code","execution_count":null,"source":["final_df = df.copy()\n","final_df['2-grams']= df['text'].apply(get_2_grams)"],"outputs":[],"metadata":{"id":"4fNdtlaUWGf-"}},{"cell_type":"markdown","source":["As we just need the probability distribution we can drop the count column:"],"metadata":{"id":"L9HxFjuBRmed"}},{"cell_type":"code","execution_count":null,"source":["new_pair_counts = pair_counts.drop(['count'], axis=1)"],"outputs":[],"metadata":{"id":"LFfTh_7GYWp3"}},{"cell_type":"markdown","source":["We did several benchmarks with the help of other friends to see which method is the most efficient. Using the vanilla DataFrame crashes the Colab as it uses too much RAM. \n","\n","Finally using the dictionary is the most efficient method."],"metadata":{"id":"S5t7xR_URsUh"}},{"cell_type":"code","execution_count":null,"source":["new_pair_counts = new_pair_counts.to_dict(orient='index')"],"outputs":[],"metadata":{"id":"Z7yHcptjQZgD"}},{"cell_type":"code","execution_count":null,"source":["def get_prob_distrib(row):\n","  emotions = np.zeros(6)\n","  for p in row:\n","    e = list(new_pair_counts[p].values())\n","    emotions += e\n","  \n","  prob_s = np.sum(emotions)\n","  emotions /= prob_s\n","  return emotions"],"outputs":[],"metadata":{"id":"BPhjlQPRX-rU"}},{"cell_type":"code","execution_count":null,"source":["prs = final_df['2-grams'].apply(get_prob_distrib)"],"outputs":[],"metadata":{"id":"Lod-euNTWtlE"}},{"cell_type":"code","execution_count":null,"source":["list_of_emotions = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']"],"outputs":[],"metadata":{"id":"xxzHLPxXVtkf"}},{"cell_type":"code","execution_count":null,"source":["dominant_feeling = prs.apply(lambda x: list_of_emotions[np.argmax(x)])"],"outputs":[],"metadata":{"id":"7RtOrtleR46L"}},{"cell_type":"code","execution_count":null,"source":["part6_final_df = pd.DataFrame({'id': df['id'], 'feeling-prs': prs, \n","                               'dominant-feeling': dominant_feeling})"],"outputs":[],"metadata":{"id":"3GxPpO3MZEYY"}},{"cell_type":"markdown","source":["And saving it to a csv file:"],"metadata":{"id":"nre2-H8lSd9J"}},{"cell_type":"code","execution_count":null,"source":["part6_final_df.to_csv('result.csv')"],"outputs":[],"metadata":{"id":"lc1dNAIZZfst"}},{"cell_type":"markdown","source":["We can zip all the results to be able to download them easier:"],"metadata":{"id":"bWLhb9kXSjPw"}},{"cell_type":"code","execution_count":185,"source":["!zip q4_results.zip result.csv words.txt"],"outputs":[{"output_type":"stream","name":"stdout","text":["updating: result.csv (deflated 61%)\n","  adding: words.txt (deflated 54%)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRXDI2PWaVf3","executionInfo":{"status":"ok","timestamp":1617395626130,"user_tz":-270,"elapsed":3779,"user":{"displayName":"ali mirferdos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi1OueG9sGfZPOEpshnIKiyStiVabJwtjvgrIbsrw=s64","userId":"16815373180648766477"}},"outputId":"85b46719-8602-4cec-8132-43815a66ceb8"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"LX-Ex9t2cRvj"}}]}